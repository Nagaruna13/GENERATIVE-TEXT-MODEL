{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c6c2ec-f756-4044-9b83-0132f18fe3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing build dependencies ... done\n",
      "Getting requirements to build wheel ... done\n",
      "Preparing metadata (pyproject.toml) ... done\n",
      "Building wheel for transformers (pyproject.toml) ... done\n"
     ]
    }
   ],
   "source": [
    "# Install Gradio for creating a simple web interface\n",
    "# Install the latest version of Hugging Face Transformers (for GPT-2)\n",
    "!pip install -q gradio\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6109bd78-79b4-4a5d-b3dc-9f5776cc0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gradio for UI, TensorFlow backend for running GPT2, and the tokenizer/model from Transformers\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12f7432-e06c-4d33-a9ab-890dcc2313f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.0/26.0 [00:00<00:00, 521B/s]\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 4.06MB/s]\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 5.41MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 14.0MB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 15.8kB/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [00:14<00:00, 67.9MB/s]\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer to convert input text into tokens\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the GPT-2 model and assign the end-of-sequence token for padding\n",
    "# This model will be used to generate text\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensures the model handles padding correctly\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5804866a-3fa9-4234-8ec6-d0daa1c2f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts user input and generates a text paragraph\n",
    "def generate_text(input_text):\n",
    "    # Convert the input text into model-readable format (tensor of tokens)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "\n",
    "    # Generate new text using beam search (more coherent than greedy or random)\n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=350,              # Maximum length of the generated text\n",
    "        num_beams=5,                 # Beam search with 5 beams (better quality)\n",
    "        no_repeat_ngram_size=2,     # Avoid repeating 2-word phrases\n",
    "        early_stopping=True         # Stop when output seems complete\n",
    "    )\n",
    "\n",
    "    # Decode the generated output into human-readable text\n",
    "    output = tokenizer.decode(\n",
    "        beam_output[0],\n",
    "        skip_special_tokens=True,   # Remove tokens like\n",
    "        clean_up_tokenization_spaces=True  # Clean extra spaces\n",
    "    )\n",
    "\n",
    "    # Return only full sentences, cleaned up\n",
    "    return \".\".join(output.split(\".\")) + \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24a4f12-1f5b-4c0d-b4c1-7e93b28b9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a textbox output component\n",
    "output_text = gr.Textbox()\n",
    "\n",
    "# Create and launch the Gradio interface\n",
    "gr.Interface(\n",
    "    fn=generate_text,         # Function to run on input\n",
    "    inputs=\"textbox\",         # Input component (user prompt)\n",
    "    outputs=output_text,      # Output component (generated paragraph)\n",
    "    title=\"ðŸ§  GENERATIVE TEXT MODEL (GPT-2)\"  # Interface title\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
